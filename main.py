#!/usr/bin/env python3
"""
DPO Implementation - Direct Preference Optimization for LLM alignment

Purpose: Implement Nova's DPO research using Hugging Face TRL
         with QLoRA for memory-efficient fine-tuning.

Usage:
    dpo-train init                     # Initialize project
    dpo-train prepare-dataset           # Prepare preference dataset
    dpo-train train                     # Run DPO training
    dpo-train evaluate                  # Evaluate DPO model
    dpo-train compare                   # Compare DPO vs baseline
    dpo-train status                    # Show status

Examples:
    dpo-train init --model meta-llama/Llama-3.1-8B
    dpo-train prepare-dataset --source hh-rlhf
    dpo-train train --epochs 3 --learning-rate 5e-7
"""

import argparse
import json
import os
import subprocess
import sys
from dataclasses import dataclass, asdict
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional

# torch is imported lazily to avoid dependency issues in non-GPU environments
try:
    import torch
except ImportError:
    torch = None


@dataclass
class TrainingConfig:
    """DPO training configuration."""
    model_name: str
    dataset_source: str
    lora_rank: int = 8
    lora_alpha: int = 16
    quantization_bits: int = 4
    beta: float = 0.1
    learning_rate: float = 5e-7
    epochs: int = 3
    batch_size: int = 4
    max_length: int = 512
    output_dir: str = "./dpo_output"

    @classmethod
    def from_file(cls, path: Path) -> 'TrainingConfig':
        """Load config from file."""
        if not path or not path.exists():
            return cls(model_name="", dataset_source="")

        with open(path) as f:
            data = json.load(f)
        return cls(**data)

    def to_file(self, path: Path):
        """Save config to file."""
        with open(path, 'w') as f:
            json.dump(asdict(self), f, indent=2)


class DPOTrainer:
    """DPO Implementation CLI."""

    def __init__(self):
        self.config_file = Path.cwd() / ".dpo_config.json"
        self.config = TrainingConfig.from_file(self.config_file)
        self.data_dir = Path.cwd() / "dpo_data"
        self.output_dir = Path.cwd() / "dpo_output"
        self._ensure_dirs()

    def _ensure_dirs(self):
        """Ensure directories exist."""
        self.data_dir.mkdir(parents=True, exist_ok=True)
        self.output_dir.mkdir(parents=True, exist_ok=True)

    def init(self, model_name: str, dataset_source: str = "hh-rlhf"):
        """Initialize DPO project."""
        # Set defaults
        self.config.model_name = model_name
        self.config.dataset_source = dataset_source

        # Save config
        self.config.to_file(self.config_file)

        # Create requirements.txt
        req_file = Path.cwd() / "dpo_requirements.txt"
        req_content = """# DPO Training Requirements

# Core
torch>=2.0.0
transformers>=4.35.0
datasets>=2.14.0

# DPO implementation
trl>=0.7.0

# QLoRA for memory efficiency
peft>=0.6.0
bitsandbytes>=0.41.0

# Evaluation
accelerate>=0.24.0
"""
        req_file.write_text(req_content)

        # Create training script template
        train_script = Path.cwd() / "dpo_train.py"
        script_content = f'''"""
DPO Training Script

Generated by dpo-train init
"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from trl import DPOTrainer, DPOConfig
from peft import LoraConfig, get_peft_model
from datasets import load_dataset
import json
import argparse

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--model", type=str, required=True)
    parser.add_argument("--dataset", type=str, required=True)
    parser.add_argument("--epochs", type=int, default={self.config.epochs})
    parser.add_argument("--batch_size", type=int, default={self.config.batch_size})
    parser.add_argument("--learning_rate", type=float, default={self.config.learning_rate})
    parser.add_argument("--beta", type=float, default={self.config.beta})
    parser.add_argument("--lora_rank", type=int, default={self.config.lora_rank})
    parser.add_argument("--lora_alpha", type=int, default={self.config.lora_alpha})
    parser.add_argument("--output_dir", type=str, default="dpo_output")
    args = parser.parse_args()

    print(f"üöÄ Starting DPO Training")
    print(f"   Model: {{args.model}}")
    print(f"   Dataset: {{args.dataset}}")
    print(f"   Epochs: {{args.epochs}}")
    print(f"   Learning Rate: {{args.learning_rate}}")
    print(f"   Beta: {{args.beta}}")
    print(f"")

    # Load model and tokenizer
    print(f"üì¶ Loading model and tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(args.model)
    model = AutoModelForCausalLM.from_pretrained(
        args.model,
        torch_dtype=torch.float16,
        device_map="auto"
    )
    tokenizer.pad_token = tokenizer.eos_token

    # Load dataset
    print(f"üìä Loading dataset...")
    dataset = load_dataset(args.dataset, split="train")

    # Process dataset to DPO format (prompt, chosen, rejected)
    def preprocess_function(examples):
        # Adapt based on dataset format
        # HH-RLHF format: {{prompt, chosen, rejected}}
        return {{
            "prompt": examples["prompt"],
            "chosen": examples["chosen"],
            "rejected": examples["rejected"]
        }}

    processed_dataset = dataset.map(preprocess_function, remove_columns=dataset.column_names)

    # Configure QLoRA
    print(f"üîß Configuring QLoRA (rank={{args.lora_rank}}, alpha={{args.lora_alpha}})...")
    lora_config = LoraConfig(
        r=args.lora_rank,
        lora_alpha=args.lora_alpha,
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
        lora_dropout=0.05,
        bias="none",
        task_type="CAUSAL_LM"
    )

    # Apply QLoRA
    model = get_peft_model(model, lora_config)

    # Configure DPO
    print(f"‚öôÔ∏è  Configuring DPO trainer...")
    dpo_config = DPOConfig(
        beta=args.beta,
        learning_rate=args.learning_rate,
        per_device_train_batch_size=args.batch_size,
        gradient_accumulation_steps=4,
        max_length={self.config.max_length},
        logging_steps=10,
        save_strategy="steps",
        save_steps=100,
        output_dir=args.output_dir,
    )

    # Initialize trainer
    print(f"üèã  Initializing DPO trainer...")
    dpo_trainer = DPOTrainer(
        model=model,
        ref_model=None,  # DPO uses same model as reference
        args=dpo_config,
        train_dataset=processed_dataset,
        tokenizer=tokenizer,
    )

    # Train
    print(f"üéØ Starting training...")
    dpo_trainer.train()

    # Save
    print(f"üíæ Saving model...")
    dpo_trainer.save_model(args.output_dir)
    tokenizer.save_pretrained(args.output_dir)

    print(f"‚úÖ Training complete!")
    print(f"   Model saved to: {{args.output_dir}}")

if __name__ == "__main__":
    main()
'''
        train_script.write_text(script_content)

        print(f"‚úÖ DPO project initialized")
        print(f"   Config: {self.config_file}")
        print(f"   Model: {model_name}")
        print(f"   Dataset: {dataset_source}")
        print(f"")
        print(f"üìù Created files:")
        print(f"   - dpo_requirements.txt")
        print(f"   - dpo_train.py")
        print(f"")
        print(f"üí° Next steps:")
        print(f"   1. Install dependencies: pip install -r dpo_requirements.txt")
        print(f"   2. Prepare dataset: dpo-train prepare-dataset")
        print(f"   3. Run training: dpo-train train")

    def prepare_dataset(self, source: str = None, limit: int = None):
        """Prepare preference dataset."""
        source = source or self.config.dataset_source

        print(f"üìä Preparing dataset...")
        print(f"   Source: {source}")

        # Dataset sources
        datasets_map = {
            "hh-rlhf": "Anthropic/hh-rlhf",
            "webgpt": "openai/webgpt_comparisons",
            "shp": "stanfordnlp/SHP",
            "synthetic": "local",  # To be generated
        }

        dataset_name = datasets_map.get(source)
        if not dataset_name:
            print(f"‚ùå Unknown dataset source: {source}")
            print(f"   Available: {', '.join(datasets_map.keys())}")
            return

        print(f"   Dataset: {dataset_name}")
        print(f"   Downloading and processing...")

        # Load and save sample
        try:
            from datasets import load_dataset

            dataset = load_dataset(dataset_name, split="train")

            # Sample if limit specified
            if limit:
                dataset = dataset.select(range(min(limit, len(dataset))))

            # Save to JSON for review
            output_file = self.data_dir / f"{source}_sample.jsonl"
            with open(output_file, 'w') as f:
                for example in dataset:
                    json.dump(example, f)
                    f.write("\\n")

            print(f"")
            print(f"‚úÖ Dataset prepared")
            print(f"   Size: {len(dataset)} examples")
            if limit:
                print(f"   Sampled: {limit} examples")
            print(f"   Saved to: {output_file}")
            print(f"")
            print(f"üí° Next: dpo-train train")

        except ImportError:
            print(f"‚ùå datasets library not installed")
            print(f"   Install with: pip install datasets")

    def train(self):
        """Run DPO training."""
        if not self.config.model_name:
            print(f"‚ùå Project not initialized")
            print(f"   Run: dpo-train init --model <model_name>")
            return

        print(f"üöÄ Starting DPO training...")
        print(f"")
        print(f"Configuration:")
        print(f"   Model: {self.config.model_name}")
        print(f"   Dataset: {self.config.dataset_source}")
        print(f"   QLoRA Rank: {self.config.lora_rank}")
        print(f"   QLoRA Alpha: {self.config.lora_alpha}")
        print(f"   Beta: {self.config.beta}")
        print(f"   Learning Rate: {self.config.learning_rate}")
        print(f"   Epochs: {self.config.epochs}")
        print(f"   Batch Size: {self.config.batch_size}")
        print(f"   Max Length: {self.config.max_length}")
        print(f"   Output: {self.config.output_dir}")
        print(f"")

        # Run training script
        try:
            cmd = [
                "python3", "dpo_train.py",
                "--model", self.config.model_name,
                "--dataset", self.config.dataset_source,
                "--epochs", str(self.config.epochs),
                "--batch_size", str(self.config.batch_size),
                "--learning_rate", str(self.config.learning_rate),
                "--beta", str(self.config.beta),
                "--lora_rank", str(self.config.lora_rank),
                "--lora_alpha", str(self.config.lora_alpha),
                "--output_dir", str(self.config.output_dir),
            ]

            print(f"üèã  Running training script...")
            result = subprocess.run(cmd, cwd=Path.cwd())

            if result.returncode == 0:
                print(f"")
                print(f"‚úÖ Training completed successfully")
                print(f"   Model saved to: {self.config.output_dir}")
                print(f"")
                print(f"üí° Next: dpo-train evaluate")
            else:
                print(f"")
                print(f"‚ùå Training failed with exit code: {result.returncode}")

        except FileNotFoundError:
            print(f"‚ùå dpo_train.py not found")
            print(f"   Run: dpo-train init first")

    def evaluate(self):
        """Evaluate DPO model."""
        if not self.output_dir.exists():
            print(f"‚ùå No trained model found in {self.output_dir}")
            print(f"   Run: dpo-train train")
            return

        print(f"üìä Evaluating DPO model...")
        print(f"")

        # Test prompts for evaluation
        test_prompts = [
            "What is the capital of France?",
            "Explain quantum computing in simple terms.",
            "Write a short poem about coding.",
            "What are the benefits of renewable energy?",
            "How does photosynthesis work?",
        ]

        # Create evaluation results file
        eval_file = self.output_dir / "evaluation_results.jsonl"

        try:
            from transformers import AutoModelForCausalLM, AutoTokenizer

            # Load DPO model
            print(f"üì¶ Loading DPO model from {self.output_dir}...")
            model = AutoModelForCausalLM.from_pretrained(
                self.output_dir,
                torch_dtype=torch.float16,
                device_map="auto"
            )
            tokenizer = AutoTokenizer.from_pretrained(self.output_dir)

            # Generate responses
            print(f"üéØ Generating responses to {len(test_prompts)} test prompts...")
            print(f"")

            results = []
            for i, prompt in enumerate(test_prompts, 1):
                print(f"   {i}/{len(test_prompts)}: {prompt[:50]}...")

                # Generate response
                inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=128)
                inputs = {k: v.to(model.device) for k, v in inputs.items()}

                with torch.no_grad():
                    outputs = model.generate(
                        **inputs,
                        max_new_tokens=150,
                        temperature=0.7,
                        do_sample=True,
                        pad_token_id=tokenizer.eos_token_id
                    )

                response = tokenizer.decode(outputs[0], skip_special_tokens=True)

                # Calculate basic quality metrics
                metrics = self._calculate_metrics(prompt, response)

                result = {
                    "prompt": prompt,
                    "response": response,
                    "metrics": metrics
                }
                results.append(result)

                print(f"      Length: {len(response.split())} words | Avg word length: {metrics['avg_word_length']:.1f}")

            # Save results
            with open(eval_file, 'w') as f:
                for result in results:
                    f.write(json.dumps(result) + '\n')

            print(f"")
            print(f"‚úÖ Evaluation complete")
            print(f"   Results saved to: {eval_file}")
            print(f"   Total prompts: {len(test_prompts)}")
            print(f"")
            print(f"üí° Next: dpo-train compare (to compare with baseline)")

        except ImportError:
            print(f"‚ùå Required libraries not installed")
            print(f"   Install with: pip install transformers torch")
        except Exception as e:
            print(f"‚ùå Evaluation failed: {e}")

    def compare(self):
        """Compare DPO vs baseline."""
        if not self.output_dir.exists():
            print(f"‚ùå No trained model found in {self.output_dir}")
            print(f"   Run: dpo-train train")
            return

        if not self.config.model_name:
            print(f"‚ùå No baseline model configured")
            print(f"   Initialize project: dpo-train init --model <model>")
            return

        print(f"üìä Comparing DPO vs baseline...")
        print(f"")

        # Test prompts for comparison
        test_prompts = [
            "What is the capital of France?",
            "Explain quantum computing in simple terms.",
            "Write a short poem about coding.",
        ]

        # Create comparison results file
        compare_file = self.output_dir / "comparison_results.jsonl"

        try:
            from transformers import AutoModelForCausalLM, AutoTokenizer
            import torch

            # Load baseline model
            print(f"üì¶ Loading baseline model: {self.config.model_name}...")
            baseline_model = AutoModelForCausalLM.from_pretrained(
                self.config.model_name,
                torch_dtype=torch.float16,
                device_map="auto"
            )
            baseline_tokenizer = AutoTokenizer.from_pretrained(self.config.model_name)
            baseline_tokenizer.pad_token = baseline_tokenizer.eos_token

            # Load DPO model
            print(f"üì¶ Loading DPO model from {self.output_dir}...")
            dpo_model = AutoModelForCausalLM.from_pretrained(
                self.output_dir,
                torch_dtype=torch.float16,
                device_map="auto"
            )
            dpo_tokenizer = AutoTokenizer.from_pretrained(self.output_dir)

            print(f"")
            print(f"üéØ Generating side-by-side responses...")
            print(f"")

            comparisons = []
            for i, prompt in enumerate(test_prompts, 1):
                print(f"   {i}/{len(test_prompts)}: {prompt[:50]}...")

                # Generate baseline response
                baseline_inputs = baseline_tokenizer(prompt, return_tensors="pt", truncation=True, max_length=128)
                baseline_inputs = {k: v.to(baseline_model.device) for k, v in baseline_inputs.items()}

                with torch.no_grad():
                    baseline_outputs = baseline_model.generate(
                        **baseline_inputs,
                        max_new_tokens=150,
                        temperature=0.7,
                        do_sample=True,
                        pad_token_id=baseline_tokenizer.eos_token_id
                    )
                baseline_response = baseline_tokenizer.decode(baseline_outputs[0], skip_special_tokens=True)

                # Generate DPO response
                dpo_inputs = dpo_tokenizer(prompt, return_tensors="pt", truncation=True, max_length=128)
                dpo_inputs = {k: v.to(dpo_model.device) for k, v in dpo_inputs.items()}

                with torch.no_grad():
                    dpo_outputs = dpo_model.generate(
                        **dpo_inputs,
                        max_new_tokens=150,
                        temperature=0.7,
                        do_sample=True,
                        pad_token_id=dpo_tokenizer.eos_token_id
                    )
                dpo_response = dpo_tokenizer.decode(dpo_outputs[0], skip_special_tokens=True)

                comparison = {
                    "prompt": prompt,
                    "baseline_response": baseline_response,
                    "dpo_response": dpo_response,
                    "baseline_length": len(baseline_response.split()),
                    "dpo_length": len(dpo_response.split())
                }
                comparisons.append(comparison)

                print(f"      Baseline: {baseline_length} words | DPO: {dpo_length} words")

            # Save comparisons
            with open(compare_file, 'w') as f:
                for comparison in comparisons:
                    f.write(json.dumps(comparison) + '\n')

            print(f"")
            print(f"‚úÖ Comparison complete")
            print(f"   Results saved to: {compare_file}")
            print(f"   Total prompts: {len(test_prompts)}")
            print(f"")
            print(f"üí° Review comparison_results.jsonl for side-by-side analysis")

        except ImportError:
            print(f"‚ùå Required libraries not installed")
            print(f"   Install with: pip install transformers torch")
        except Exception as e:
            print(f"‚ùå Comparison failed: {e}")

    def _calculate_metrics(self, prompt: str, response: str) -> Dict:
        """Calculate basic quality metrics."""
        words = response.split()
        if not words:
            return {"length": 0, "avg_word_length": 0, "exclamation_count": 0, "question_count": 0}

        return {
            "length": len(words),
            "avg_word_length": sum(len(w) for w in words) / len(words),
            "exclamation_count": response.count('!'),
            "question_count": response.count('?')
        }

    def status(self):
        """Show status."""
        # Reload config from file to get latest values
        self.config = TrainingConfig.from_file(self.config_file)

        print(f"üìä DPO Implementation Status")
        print(f"")

        print(f"Config file: {self.config_file}")
        print(f"  Exists: {self.config_file.exists()}")
        print(f"")

        print(f"Configuration:")
        print(f"   Model: {self.config.model_name or 'Not set'}")
        print(f"   Dataset: {self.config.dataset_source or 'Not set'}")
        print(f"   QLoRA Rank: {self.config.lora_rank}")
        print(f"   QLoRA Alpha: {self.config.lora_alpha}")
        print(f"   Beta: {self.config.beta}")
        print(f"   Learning Rate: {self.config.learning_rate}")
        print(f"   Epochs: {self.config.epochs}")
        print(f"   Output: {self.config.output_dir}")

        print(f"")
        print(f"Data directory: {self.data_dir}")
        print(f"  Exists: {self.data_dir.exists()}")
        if self.data_dir.exists():
            datasets = list(self.data_dir.glob("*_sample.jsonl"))
            print(f"  Datasets prepared: {len(datasets)}")
            for d in datasets:
                print(f"    - {d.name}")

        print(f"")
        print(f"Output directory: {self.output_dir}")
        print(f"  Exists: {self.output_dir.exists()}")
        if self.output_dir.exists():
            checkpoints = list(self.output_dir.glob("checkpoint-*"))
            print(f"  Checkpoints: {len(checkpoints)}")

        print(f"")
        print(f"Training script: dpo_train.py")
        train_script_path = Path.cwd() / "dpo_train.py"
        print(f"  Exists: {train_script_path.exists()}")


def main():
    """Main CLI entry point."""
    parser = argparse.ArgumentParser(
        description="DPO Implementation - Direct Preference Optimization for LLM alignment"
    )
    parser.add_argument(
        "--version", action="version", version="DPO Implementation 1.0"
    )

    subparsers = parser.add_subparsers(dest="command", help="Available commands")

    # init command
    init_parser = subparsers.add_parser("init", help="Initialize DPO project")
    init_parser.add_argument("--model", required=True, help="Base model name (e.g., meta-llama/Llama-3.1-8B)")
    init_parser.add_argument("--dataset", default="hh-rlhf", help="Dataset source (hh-rlhf, webgpt, shp)")

    # prepare-dataset command
    prep_parser = subparsers.add_parser("prepare-dataset", help="Prepare preference dataset")
    prep_parser.add_argument("--source", help="Dataset source (default from config)")
    prep_parser.add_argument("--limit", type=int, help="Limit dataset size for testing")

    # train command
    train_parser = subparsers.add_parser("train", help="Run DPO training")
    train_parser.add_argument("--epochs", type=int, help="Number of epochs")
    train_parser.add_argument("--learning-rate", type=float, help="Learning rate")
    train_parser.add_argument("--beta", type=float, help="DPO beta parameter")

    # evaluate command
    subparsers.add_parser("evaluate", help="Evaluate DPO model")

    # compare command
    subparsers.add_parser("compare", help="Compare DPO vs baseline")

    # status command
    subparsers.add_parser("status", help="Show status")

    args = parser.parse_args()

    if not args.command:
        parser.print_help()
        return

    trainer = DPOTrainer()

    if args.command == "init":
        trainer.init(args.model, args.dataset)
    elif args.command == "prepare-dataset":
        trainer.prepare_dataset(args.source, args.limit)
    elif args.command == "train":
        trainer.train()
    elif args.command == "evaluate":
        trainer.evaluate()
    elif args.command == "compare":
        trainer.compare()
    elif args.command == "status":
        trainer.status()


if __name__ == "__main__":
    main()
