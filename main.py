#!/usr/bin/env python3
"""
DPO Implementation - Direct Preference Optimization for LLM alignment

Purpose: Implement Nova's DPO research using Hugging Face TRL
         with QLoRA for memory-efficient fine-tuning.

Usage:
    dpo-train init                     # Initialize project
    dpo-train prepare-dataset           # Prepare preference dataset
    dpo-train train                     # Run DPO training
    dpo-train evaluate                  # Evaluate DPO model
    dpo-train compare                   # Compare DPO vs baseline
    dpo-train status                    # Show status

Examples:
    dpo-train init --model meta-llama/Llama-3.1-8B
    dpo-train prepare-dataset --source hh-rlhf
    dpo-train train --epochs 3 --learning-rate 5e-7
"""

import argparse
import json
import os
import subprocess
import sys
from dataclasses import dataclass, asdict
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional


@dataclass
class TrainingConfig:
    """DPO training configuration."""
    model_name: str
    dataset_source: str
    lora_rank: int = 8
    lora_alpha: int = 16
    quantization_bits: int = 4
    beta: float = 0.1
    learning_rate: float = 5e-7
    epochs: int = 3
    batch_size: int = 4
    max_length: int = 512
    output_dir: str = "./dpo_output"

    @classmethod
    def from_file(cls, path: Path) -> 'TrainingConfig':
        """Load config from file."""
        if not path or not path.exists():
            return cls(model_name="", dataset_source="")

        with open(path) as f:
            data = json.load(f)
        return cls(**data)

    def to_file(self, path: Path):
        """Save config to file."""
        with open(path, 'w') as f:
            json.dump(asdict(self), f, indent=2)


class DPOTrainer:
    """DPO Implementation CLI."""

    def __init__(self):
        self.config_file = Path.cwd() / ".dpo_config.json"
        self.config = TrainingConfig.from_file(self.config_file)
        self.data_dir = Path.cwd() / "dpo_data"
        self.output_dir = Path.cwd() / "dpo_output"
        self._ensure_dirs()

    def _ensure_dirs(self):
        """Ensure directories exist."""
        self.data_dir.mkdir(parents=True, exist_ok=True)
        self.output_dir.mkdir(parents=True, exist_ok=True)

    def init(self, model_name: str, dataset_source: str = "hh-rlhf"):
        """Initialize DPO project."""
        # Set defaults
        self.config.model_name = model_name
        self.config.dataset_source = dataset_source

        # Save config
        self.config.to_file(self.config_file)

        # Create requirements.txt
        req_file = Path.cwd() / "dpo_requirements.txt"
        req_content = """# DPO Training Requirements

# Core
torch>=2.0.0
transformers>=4.35.0
datasets>=2.14.0

# DPO implementation
trl>=0.7.0

# QLoRA for memory efficiency
peft>=0.6.0
bitsandbytes>=0.41.0

# Evaluation
accelerate>=0.24.0
"""
        req_file.write_text(req_content)

        # Create training script template
        train_script = Path.cwd() / "dpo_train.py"
        script_content = f'''"""
DPO Training Script

Generated by dpo-train init
"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from trl import DPOTrainer, DPOConfig
from peft import LoraConfig, get_peft_model
from datasets import load_dataset
import json
import argparse

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--model", type=str, required=True)
    parser.add_argument("--dataset", type=str, required=True)
    parser.add_argument("--epochs", type=int, default={self.config.epochs})
    parser.add_argument("--batch_size", type=int, default={self.config.batch_size})
    parser.add_argument("--learning_rate", type=float, default={self.config.learning_rate})
    parser.add_argument("--beta", type=float, default={self.config.beta})
    parser.add_argument("--lora_rank", type=int, default={self.config.lora_rank})
    parser.add_argument("--lora_alpha", type=int, default={self.config.lora_alpha})
    parser.add_argument("--output_dir", type=str, default="dpo_output")
    args = parser.parse_args()

    print(f"ğŸš€ Starting DPO Training")
    print(f"   Model: {{args.model}}")
    print(f"   Dataset: {{args.dataset}}")
    print(f"   Epochs: {{args.epochs}}")
    print(f"   Learning Rate: {{args.learning_rate}}")
    print(f"   Beta: {{args.beta}}")
    print(f"")

    # Load model and tokenizer
    print(f"ğŸ“¦ Loading model and tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(args.model)
    model = AutoModelForCausalLM.from_pretrained(
        args.model,
        torch_dtype=torch.float16,
        device_map="auto"
    )
    tokenizer.pad_token = tokenizer.eos_token

    # Load dataset
    print(f"ğŸ“Š Loading dataset...")
    dataset = load_dataset(args.dataset, split="train")

    # Process dataset to DPO format (prompt, chosen, rejected)
    def preprocess_function(examples):
        # Adapt based on dataset format
        # HH-RLHF format: {{prompt, chosen, rejected}}
        return {{
            "prompt": examples["prompt"],
            "chosen": examples["chosen"],
            "rejected": examples["rejected"]
        }}

    processed_dataset = dataset.map(preprocess_function, remove_columns=dataset.column_names)

    # Configure QLoRA
    print(f"ğŸ”§ Configuring QLoRA (rank={{args.lora_rank}}, alpha={{args.lora_alpha}})...")
    lora_config = LoraConfig(
        r=args.lora_rank,
        lora_alpha=args.lora_alpha,
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
        lora_dropout=0.05,
        bias="none",
        task_type="CAUSAL_LM"
    )

    # Apply QLoRA
    model = get_peft_model(model, lora_config)

    # Configure DPO
    print(f"âš™ï¸  Configuring DPO trainer...")
    dpo_config = DPOConfig(
        beta=args.beta,
        learning_rate=args.learning_rate,
        per_device_train_batch_size=args.batch_size,
        gradient_accumulation_steps=4,
        max_length={self.config.max_length},
        logging_steps=10,
        save_strategy="steps",
        save_steps=100,
        output_dir=args.output_dir,
    )

    # Initialize trainer
    print(f"ğŸ‹  Initializing DPO trainer...")
    dpo_trainer = DPOTrainer(
        model=model,
        ref_model=None,  # DPO uses same model as reference
        args=dpo_config,
        train_dataset=processed_dataset,
        tokenizer=tokenizer,
    )

    # Train
    print(f"ğŸ¯ Starting training...")
    dpo_trainer.train()

    # Save
    print(f"ğŸ’¾ Saving model...")
    dpo_trainer.save_model(args.output_dir)
    tokenizer.save_pretrained(args.output_dir)

    print(f"âœ… Training complete!")
    print(f"   Model saved to: {{args.output_dir}}")

if __name__ == "__main__":
    main()
'''
        train_script.write_text(script_content)

        print(f"âœ… DPO project initialized")
        print(f"   Config: {self.config_file}")
        print(f"   Model: {model_name}")
        print(f"   Dataset: {dataset_source}")
        print(f"")
        print(f"ğŸ“ Created files:")
        print(f"   - dpo_requirements.txt")
        print(f"   - dpo_train.py")
        print(f"")
        print(f"ğŸ’¡ Next steps:")
        print(f"   1. Install dependencies: pip install -r dpo_requirements.txt")
        print(f"   2. Prepare dataset: dpo-train prepare-dataset")
        print(f"   3. Run training: dpo-train train")

    def prepare_dataset(self, source: str = None, limit: int = None):
        """Prepare preference dataset."""
        source = source or self.config.dataset_source

        print(f"ğŸ“Š Preparing dataset...")
        print(f"   Source: {source}")

        # Dataset sources
        datasets_map = {
            "hh-rlhf": "Anthropic/hh-rlhf",
            "webgpt": "openai/webgpt_comparisons",
            "shp": "stanfordnlp/SHP",
            "synthetic": "local",  # To be generated
        }

        dataset_name = datasets_map.get(source)
        if not dataset_name:
            print(f"âŒ Unknown dataset source: {source}")
            print(f"   Available: {', '.join(datasets_map.keys())}")
            return

        print(f"   Dataset: {dataset_name}")
        print(f"   Downloading and processing...")

        # Load and save sample
        try:
            from datasets import load_dataset

            dataset = load_dataset(dataset_name, split="train")

            # Sample if limit specified
            if limit:
                dataset = dataset.select(range(min(limit, len(dataset))))

            # Save to JSON for review
            output_file = self.data_dir / f"{source}_sample.jsonl"
            with open(output_file, 'w') as f:
                for example in dataset:
                    json.dump(example, f)
                    f.write("\\n")

            print(f"")
            print(f"âœ… Dataset prepared")
            print(f"   Size: {len(dataset)} examples")
            if limit:
                print(f"   Sampled: {limit} examples")
            print(f"   Saved to: {output_file}")
            print(f"")
            print(f"ğŸ’¡ Next: dpo-train train")

        except ImportError:
            print(f"âŒ datasets library not installed")
            print(f"   Install with: pip install datasets")

    def train(self):
        """Run DPO training."""
        if not self.config.model_name:
            print(f"âŒ Project not initialized")
            print(f"   Run: dpo-train init --model <model_name>")
            return

        print(f"ğŸš€ Starting DPO training...")
        print(f"")
        print(f"Configuration:")
        print(f"   Model: {self.config.model_name}")
        print(f"   Dataset: {self.config.dataset_source}")
        print(f"   QLoRA Rank: {self.config.lora_rank}")
        print(f"   QLoRA Alpha: {self.config.lora_alpha}")
        print(f"   Beta: {self.config.beta}")
        print(f"   Learning Rate: {self.config.learning_rate}")
        print(f"   Epochs: {self.config.epochs}")
        print(f"   Batch Size: {self.config.batch_size}")
        print(f"   Max Length: {self.config.max_length}")
        print(f"   Output: {self.config.output_dir}")
        print(f"")

        # Run training script
        try:
            cmd = [
                "python3", "dpo_train.py",
                "--model", self.config.model_name,
                "--dataset", self.config.dataset_source,
                "--epochs", str(self.config.epochs),
                "--batch_size", str(self.config.batch_size),
                "--learning_rate", str(self.config.learning_rate),
                "--beta", str(self.config.beta),
                "--lora_rank", str(self.config.lora_rank),
                "--lora_alpha", str(self.config.lora_alpha),
                "--output_dir", str(self.config.output_dir),
            ]

            print(f"ğŸ‹  Running training script...")
            result = subprocess.run(cmd, cwd=Path.cwd())

            if result.returncode == 0:
                print(f"")
                print(f"âœ… Training completed successfully")
                print(f"   Model saved to: {self.config.output_dir}")
                print(f"")
                print(f"ğŸ’¡ Next: dpo-train evaluate")
            else:
                print(f"")
                print(f"âŒ Training failed with exit code: {result.returncode}")

        except FileNotFoundError:
            print(f"âŒ dpo_train.py not found")
            print(f"   Run: dpo-train init first")

    def evaluate(self):
        """Evaluate DPO model."""
        print(f"ğŸ“Š Evaluating DPO model...")
        print(f"   This is a placeholder for evaluation")
        print(f"   Implement human evaluation and automated metrics")

    def compare(self):
        """Compare DPO vs baseline."""
        print(f"ğŸ“Š Comparing DPO vs baseline...")
        print(f"   This is a placeholder for comparison")
        print(f"   Implement side-by-side sample generation")

    def status(self):
        """Show status."""
        print(f"ğŸ“Š DPO Implementation Status")
        print(f"")

        print(f"Config file: {self.config_file}")
        if self.config_file.exists():
            print(f"")
            print(f"Configuration:")
            print(f"   Model: {self.config.model_name or 'Not set'}")
            print(f"   Dataset: {self.config.dataset_source or 'Not set'}")
            print(f"   QLoRA Rank: {self.config.lora_rank}")
            print(f"   QLoRA Alpha: {self.config.lora_alpha}")
            print(f"   Beta: {self.config.beta}")
            print(f"   Learning Rate: {self.config.learning_rate}")
            print(f"   Epochs: {self.config.epochs}")
            print(f"   Output: {self.config.output_dir}")

        print(f"")
        print(f"Data directory: {self.data_dir}")
        print(f"  Exists: {self.data_dir.exists()}")
        if self.data_dir.exists():
            datasets = list(self.data_dir.glob("*_sample.jsonl"))
            print(f"  Datasets prepared: {len(datasets)}")
            for d in datasets:
                print(f"    - {d.name}")

        print(f"")
        print(f"Output directory: {self.output_dir}")
        print(f"  Exists: {self.output_dir.exists()}")
        if self.output_dir.exists():
            checkpoints = list(self.output_dir.glob("checkpoint-*"))
            print(f"  Checkpoints: {len(checkpoints)}")

        print(f"")
        print(f"Training script: dpo_train.py")
        train_script_path = Path.cwd() / "dpo_train.py"
        print(f"  Exists: {train_script_path.exists()}")


def main():
    """Main CLI entry point."""
    parser = argparse.ArgumentParser(
        description="DPO Implementation - Direct Preference Optimization for LLM alignment"
    )
    parser.add_argument(
        "--version", action="version", version="DPO Implementation 1.0"
    )

    subparsers = parser.add_subparsers(dest="command", help="Available commands")

    # init command
    init_parser = subparsers.add_parser("init", help="Initialize DPO project")
    init_parser.add_argument("--model", required=True, help="Base model name (e.g., meta-llama/Llama-3.1-8B)")
    init_parser.add_argument("--dataset", default="hh-rlhf", help="Dataset source (hh-rlhf, webgpt, shp)")

    # prepare-dataset command
    prep_parser = subparsers.add_parser("prepare-dataset", help="Prepare preference dataset")
    prep_parser.add_argument("--source", help="Dataset source (default from config)")
    prep_parser.add_argument("--limit", type=int, help="Limit dataset size for testing")

    # train command
    train_parser = subparsers.add_parser("train", help="Run DPO training")
    train_parser.add_argument("--epochs", type=int, help="Number of epochs")
    train_parser.add_argument("--learning-rate", type=float, help="Learning rate")
    train_parser.add_argument("--beta", type=float, help="DPO beta parameter")

    # evaluate command
    subparsers.add_parser("evaluate", help="Evaluate DPO model")

    # compare command
    subparsers.add_parser("compare", help="Compare DPO vs baseline")

    # status command
    subparsers.add_parser("status", help="Show status")

    args = parser.parse_args()

    if not args.command:
        parser.print_help()
        return

    trainer = DPOTrainer()

    if args.command == "init":
        trainer.init(args.model, args.dataset)
    elif args.command == "prepare-dataset":
        trainer.prepare_dataset(args.source, args.limit)
    elif args.command == "train":
        trainer.train()
    elif args.command == "evaluate":
        trainer.evaluate()
    elif args.command == "compare":
        trainer.compare()
    elif args.command == "status":
        trainer.status()


if __name__ == "__main__":
    main()
